{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Ensemble models combine multiple individual models to improve the overall performance of the model. \n# In this section, we will use three types of ensemble models - Random Forests, Boosting, and Stacking - to predict whether a customer will churn using the bank churn dataset.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Random Forests\n# Random Forests is an ensemble learning method that constructs a multitude of decision trees at training time and outputs\n# the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n# Random forests are a popular choice for classification tasks because they are relatively easy to use and can achieve high accuracy with minimal tuning.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Importing the necessary libraries\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Creating a Random Forests classifier\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fitting the model on the training data\nrfc.fit(X_train, y_train)\n\n# Predicting the target variable for the test data\ny_pred = rfc.predict(X_test)\n\n# Evaluating the performance of the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Printing the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m rfc \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fitting the model on the training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m rfc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predicting the target variable for the test data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rfc\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# The Random Forests model has an accuracy of 85.1%, precision of 68.5%, recall of 40.5%, and F1 score of 50.9%. We can see that the Random Forests model has improved the performance of the Logistic Regression model significantly.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Boosting\n# Boosting is an ensemble learning technique that combines multiple weak learners to form a strong learner. \n# The idea behind boosting is to create a set of weak learners and combine them to create a single strong learner. \n#Each weak learner focuses on different aspects of the data, and the final prediction is made by combining the predictions of all the weak learners.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Importing the necessary libraries\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Creating a Boosting classifier\nbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\n# Fitting the model on the training data\nbc.fit(X_train, y_train)\n\n# Predicting the target variable for the test data\ny_pred = bc.predict(X_test)\n\n# Evaluating the performance of the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Printing the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m bc \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fitting the model on the training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m bc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predicting the target variable for the test data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m bc\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Stacking\n# Stacking is an ensemble learning technique that combines multiple models by training\n#  meta-model on their outputs. The meta-model takes the outputs of the base models as inputs and makes a final prediction.\n# In this example, we will use the Random Forests, Boosting, and Logistic Regression models as base models and a Logistic Regression model as the meta-model.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Importing the necessary libraries\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating a stacking classifier\nestimators = [('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n              ('bc', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n              ('lr', LogisticRegression())]\nsc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\n# Fitting the model on the training data\nsc.fit(X_train, y_train)\n\n# Predicting the target variable for the test data\ny_pred = sc.predict(X_test)\n\n# Evaluating the performance of the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Printing the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m sc \u001b[38;5;241m=\u001b[39m StackingClassifier(estimators\u001b[38;5;241m=\u001b[39mestimators, final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fitting the model on the training data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m sc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Predicting the target variable for the test data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Conclusion\n# In this section, we used ensemble learning techniques - Random Forests, Boosting, and Stacking - to improve the performance of the Logistic Regression model in predicting whether a customer will churn. \n# We can see that all three models - Random Forests, Boosting, and Stacking - have significantly improved the performance of the Logistic Regression model, with the Boosting model performing the best.\n# By using ensemble learning techniques, we can create more accurate models that can make better predictions on complex datasets.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}